---
title: "Inference and Modeling Notes"
author: "natsuMishi"
date: "2023-12-28"
output: html_document
---

# Section 1: Parameters and Estimates

# Section 1 Overview

Section 1 introduces you to parameters and estimates.

After completing Section 1, you will be able to:

-   Understand how to use a sampling model to perform a poll.

-   Explain the terms **population**, **parameter**, and **sample** as they relate to statistical inference.

-   Use a sample to estimate the population proportion from the sample average.

-   Calculate the expected value and standard error of the sample average.

# Sampling Model Parameters and Estimates

-   The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.

<!-- -->

-   In a sampling model, the collection of elements in the urn is called the *population.*

<!-- -->

-   A *parameter* is a number that summarizes data for an entire population.

<!-- -->

-   A *sample* is observed data from a subset of the population.

<!-- -->

-   An *estimate* is a summary of the observed data about a parameter that we believe is informative. It is a data-driven guess of the population parameter.

![](images/clipboard-3095055459.png)

-   The sample proportion is a random variable. Sampling gives random results drawn from the population distribution.

### Code: Function for taking a random draw from a specific urn

The **dslabs** package includes a function for taking a random draw of size n from the urn described in the video:

```{r}
library(tidyverse)
library(dslabs)
ds_theme_set()
take_poll(25)    # draw 25 beads
```

### The Sample Average

![](images/clipboard-1596829150.png)

### Polling versus Forecasting

-   A poll taken in advance of an election estimates p for that moment, not for election day.

-   In order to predict election results, forecasters try to use early estimates of p to predict p on election day. We discuss some approaches in later sections.

### Properties of Our Estimate

![![](images/clipboard-610903079.png){width="237"}](images/clipboard-1317090737.png)

![](images/clipboard-1002058319.png)

# Assessment 1.1: Parameters and Estimates

## Exercise 1. Polling - expected value of S

1\. Suppose you poll a population in which a proportion p of voters are Democrats and 1−p are Republicans. Your sample size is N=25. Consider the random variable S which is the **total** number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of p.

Ans. E(S) = 25 \* p

## Exercise 2. Polling - standard error of S

![](images/clipboard-3851961928.png)

2\. What is the standard error of S ? Hint: it’s a function of p.

![](images/clipboard-2080617049.png){width="181"}

## Exercise 3. Polling - expected value of X-bar

![![](images/clipboard-1955129058.png){width="122"}](images/clipboard-2815746691.png)

3\. Consider the random variable S/N. This is equivalent to the sample average, which we have been denoting as ¯X. What is the expected value of the ¯X? Hint: it’s a function of p.

## Exercise 4. Polling - standard error of X-bar

![](images/clipboard-3808667543.png)

![](images/clipboard-378992676.png){width="126"}

4\. What is the standard error of ¯X? Hint: it’s a function of p.

## Exercise 5. se versus p

Write a line of code that calculates the standard error `se` of a sample average when you poll 25 people in the population. Generate a sequence of 100 proportions of Democrats `p` that vary from 0 (no Democrats) to 1 (all Democrats).

Plot `se` versus `p` for the 100 different proportions.

-   Use the `seq` function to generate a vector of 100 values of `p` that range from 0 to 1.

-   Use the `sqrt` function to generate a vector of standard errors for all values of `p`.

-   Use the `plot` function to generate a plot with `p` on the x-axis and `se` on the y-axis.

```{r}
# `N` represents the number of people polled
N <- 25

# Create a variable `p` that contains 100 proportions ranging from 0 to 1 using the `seq` function
p <- seq(0,1, length.out = 100)
p

# Create a variable `se` that contains the standard error of each sample average
se <- sqrt(p*(1-p)/N)
se
# Plot `p` on the x-axis and `se` on the y-axis
plot(p, se)
```

## Exercise 6. Multiple plots of se versus p

![](images/clipboard-1439325597.png)

-   Your for-loop should contain two lines of code to be repeated for three different values of N.

-   The first line within the for-loop should use the `sqrt` function to generate a vector of standard errors `se` for all values of `p`.

-   The second line within the for-loop should use the `plot` function to generate a plot with `p` on the x-axis and `se` on the y-axis.

-   Use the `ylim` argument to keep the y-axis limits constant across all three plots. The lower limit should be equal to 0 and the upper limit should equal 0.1 (it can be shown that this value is the highest calculated standard error across all values of `p` and `N`).

```{r}
# The vector `p` contains 100 proportions of Democrats ranging from 0 to 1 using the `seq` function
p <- seq(0, 1, length = 100)

# The vector `sample_sizes` contains the three sample sizes
sample_sizes <- c(25, 100, 1000)

# Write a for-loop that calculates the standard error `se` for every value of `p` for each of the three samples sizes `N` in the vector `sample_sizes`. Plot the three graphs, using the `ylim` argument to standardize the y-axis across all three plots.
for(n in sample_sizes){
    se <- sqrt(p*(1-p)/sample_sizes)
    plot(p, se, ylim = c(0, 0.1))
}
```

## Exercise 7. Expected value of d

![](images/clipboard-847696336.png)

![](images/clipboard-1315534093.png)

## Exercise 8. Standard error of d

![](images/clipboard-3313486290.png)

![](images/clipboard-2452175276.png)

## Exercise 9. Standard error of the spread

![](images/clipboard-1801572909.png)

```{r}
# `N` represents the number of people polled
N <- 25

# `p` represents the proportion of Democratic voters
p <- 0.45

# Calculate the standard error of the spread. Print this value to the console.
2*sqrt(p*(1-p)/N)
```

## Exercise 10. Sample size

![](images/clipboard-2372263968.png)

![](images/clipboard-2689455482.png)

# Section 2 Overview

![](images/clipboard-4246266515.png)

### The Central Limit Theorem in Practice

![](images/clipboard-4023891576.png)

plug-in estimate

### Code: Computing the probability of Xbar being within .01 of p

```{r}
X_hat <- 0.48
se <- sqrt(X_hat*(1-X_hat)/25)
pnorm(0.01/se) - pnorm(-0.01/se)
```

### Margin of Error

![](images/clipboard-2680709885.png)

### A Monte Carlo Simulation for the CLT

-   We can run Monte Carlo simulations to compare with theoretical results assuming a value of p.

<!-- -->

-   In practice, is unknown. We can corroborate theoretical results by running Monte Carlo simulations with one or several values of p.

We don't know p, but we could run it for various values of p and sample sizes N and see that the theory works well for most values.

-   One practical choice for p when modeling is Xbar, the observed value of in a sample Xhat.![](images/clipboard-2180675967.png)

### Code: Monte Carlo simulation using a set value of p

```{r}
p <- 0.45    # unknown p to estimate
N <- 1000

# simulate one poll of size N and determine x_hat
x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)

# simulate B polls of size N and determine average x_hat
B <- 10000    # number of replicates
N <- 1000    # sample size per replicate
x_hat <- replicate(B, {
    x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    mean(x)
})
```

### Code: Histogram and QQ-plot of Monte Carlo results

```{r}
library(tidyverse)
library(gridExtra)
p1 <- data.frame(x_hat = x_hat) %>%
    ggplot(aes(x_hat)) +
    geom_histogram(binwidth = 0.005, color = "black")
p2 <- data.frame(x_hat = x_hat) %>%
    ggplot(aes(sample = x_hat)) +
    stat_qq(dparams = list(mean = mean(x_hat), sd = sd(x_hat))) +
    geom_abline() +
    ylab("X_hat") +
    xlab("Theoretical normal")
grid.arrange(p1, p2, nrow=1)
```

### The Spread

![](images/clipboard-2485748537.png)

### Bias: Why Not Run a Very Large Poll?

-   An extremely large poll would theoretically be able to predict election results almost perfectly.

-   These sample sizes are not practical. In addition to cost concerns, polling doesn't reach everyone in the population (eventual voters) with equal probability, and it also may include data from outside our population (people who will not end up voting).

-   These systematic errors in polling are called *bias*. We will learn more about bias in the future.

### Code: Plotting margin of error in an extremely large poll over a range of values of p

```{r}
library(tidyverse)
N <- 100000
p <- seq(0.35, 0.65, length = 100)
SE <- sapply(p, function(x) 2*sqrt(x*(1-x)/N))
data.frame(p = p, SE = SE) %>%
    ggplot(aes(p, SE)) +
    geom_line()
```

# Assessment 2.1: Introduction to Inference

## Exercise 1. Sample average

Write function called `take_sample` that takes the proportion of Democrats p and the sample size N as arguments and returns the sample average of Democrats (1) and Republicans (0).

Calculate the sample average if the proportion of Democrats equals 0.45 and the sample size is 100.

![](images/clipboard-1522920268.png)

```{r}
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
take_sample <- function(p,N){
    sim <- sample(c(1,0), N, replace = TRUE, prob = c(p, 1-p))   
    mean(sim)
}

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
take_sample(p, N)
```

## Exercise 2. Distribution of errors - 1

![](images/clipboard-1445621711.png)

-   The function `take_sample` that you defined in the previous exercise has already been run for you.

<!-- -->

-   Use the `replicate` function to replicate subtracting the result of `take_sample` from the value of p 10,000 times.

<!-- -->

-   Use the `mean` function to calculate the average of the differences between the sample average and actual value of p.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
errors <- replicate(B, {
    differences <- p - take_sample(p,N)
    differences
})

# Calculate the mean of the errors. Print this value to the console.
mean(errors)
```

## Exercise 3. Distribution of errors - 2

![](images/clipboard-2189136323.png)

The `errors` object has already been loaded for you. Use the `hist` function to plot a histogram of the values contained in the vector `errors`. Which statement best describes the distribution of the errors?

![![](images/clipboard-1090704458.png)](images/clipboard-3251208375.png)

## Exercise 4. Average size of error

![](images/clipboard-162436746.png)

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
mean(abs(errors))

```

## Exercise 5. Standard deviation of the spread

The standard error is related to the typical **size** of the error we make when predicting. We say **size** because, as we just saw, the errors are centered around 0. In that sense, the typical error is 0. For mathematical reasons related to the central limit theorem, we actually use the standard deviation of `errors` rather than the average of the absolute values.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the standard deviation of `errors`
sqrt(mean((errors)^2))

```

## Exercise 6. Estimating the standard error

![](images/clipboard-2893701149.png)

-   Calculate the standard error using the `sqrt` function

```{r}
# Define `p` as the expected value equal to 0.45
p <- 0.45

# Define `N` as the sample size
N <- 100

# Calculate the standard error
sqrt(p*(1-p)/N)
```

## Exercise 7. Standard error of the estimate

![](images/clipboard-3509517342.png)

-   Simulate a poll `X` using the `sample` function.

-   When using the `sample` function, create a vector using `c()` that contains all possible polling options where '1' indicates a Democratic voter and '0' indicates a Republican voter.

-   When using the `sample` function, use `replace = TRUE` within the `sample` function to indicate that sampling from the vector should occur with replacement.

-   When using the `sample` function, use `prob =` within the `sample` function to indicate the probabilities of selecting either element (0 or 1) within the vector of possibilities.

-   Use the `mean` function to calculate the average of the simulated poll, `X_bar`.

-   Calculate the standard error of the `X_bar` using the `sqrt` function and print the result.

```{r}
# Define `p` as a proportion of Democratic voters to simulate
p <- 0.45

# Define `N` as the sample size
N <- 100

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `X` as a random sample of `N` voters with a probability of picking a Democrat ('1') equal to `p`
X <- sample(c(1,0), N, replace = TRUE, prob = c(p, 1-p))
X
# Define `X_bar` as the average sampled proportion
X_bar <- mean(X)
X_bar
# Calculate the standard error of the estimate. Print the result to the console.
sqrt((X_bar * (1-X_bar))/N)

```

## Exercise 8. Plotting the standard error

![](images/clipboard-1311434742.png)

```{r}
N <- seq(100, 5000, len = 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)
plot(se, N)
```

![![](images/clipboard-2298647615.png)](images/clipboard-1973722717.png)

## Exercise 9. Distribution of X-hat

![](images/clipboard-2372815898.png)

![](images/clipboard-2438765111.png)

## Exercise 10. Distribution of the errors

![](images/clipboard-705616515.png)

![](images/clipboard-2213686483.png)

## Exercise 11. Plotting the errors

Make a qq-plot of the `errors` you generated previously to see if they follow a normal distribution.

-   Run the supplied code

-   Use the `qqnorm` function to produce a qq-plot of the errors.

-   Use the `qqline` function to plot a line showing a normal distribution.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Generate `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Generate a qq-plot of `errors` with a qq-line showing a normal distribution
qqnorm(errors)
qqline(errors)
```

## Exercise 12. Estimating the probability of a specific value of X-bar

![](images/clipboard-1196500618.png)

-   Use `pnorm` to define the probability that a value will be greater than 0.5.

![](images/clipboard-3536925196.png)

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Calculate the probability that the estimated proportion of Democrats in the population is greater than 0.5. Print this value to the console.
1-pnorm(0.5, p, sqrt(p*(1-p)/N))
```

## Exercise 13. Estimating the probability of a specific error size

![](images/clipboard-5542308.png)

-   Calculate the standard error of the sample average using the `sqrt` function.

-   Use `pnorm` twice to define the probabilities that a value will be less than -0.01 or greater than 0.01.

-   Combine these results to calculate the probability that the error *size* will be 0.01 or larger.

```{r}
# Define `N` as the number of people polled
N <-100

# Define `X_hat` as the sample average
X_hat <- 0.51

# Define `se_hat` as the standard error of the sample average
se_hat <- sqrt(X_hat*(1-X_hat)/N)

# Calculate the probability that the error is 0.01 or larger
1-(pnorm(0.01/se_hat) - pnorm(-0.01/se_hat))
```

Note: CLT works if Xbar i used in place of p. Plug-in estimate.

# Section 3 Overview

In Section 3, you will look at confidence intervals and p-values.

After completing Section 3, you will be able to:

-   Calculate confidence intervals of difference sizes around an estimate.

-   Understand that a confidence interval is a random interval with the given probability of falling on top of the parameter.

-   Explain the concept of "power" as it relates to inference.

-   Understand the relationship between p-values and confidence intervals and explain why reporting confidence intervals is often preferable.

## Confidence Intervals

![](images/clipboard-3124435332.png)

### Code: geom_smooth confidence interval example

The shaded area around the curve is related to the concept of confidence intervals.

```{r}
library(dplyr)
library(dslabs)
library(ggplot2)
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
    ggplot(aes(year, temperature)) +
    geom_point() +
    geom_smooth() +
    ggtitle("Average Yearly Temperatures in New Haven")
```

### Code: Monte Carlo simulation of confidence intervals

Note that to compute the exact 95% confidence interval, we would use qnorm(.975)\*SE_hat instead of 2\*SE_hat.

```{r}
p <- 0.45
N <- 1000
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))    # generate N observations
X_hat <- mean(X)    # calculate X_hat
SE_hat <- sqrt(X_hat*(1-X_hat)/N)    # calculate SE_hat, SE of the mean of N observations
c(X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # build interval of 2*SE above and below mean
```

### Code: Solving for z with `qnorm`

```{r}
z <- qnorm(0.995)    # calculate z to solve for 99% confidence interval
pnorm(qnorm(0.995))    # demonstrating that qnorm gives the z value for a given probability
pnorm(qnorm(1-0.995))    # demonstrating symmetry of 1-qnorm
pnorm(z) - pnorm(-z)    # demonstrating that this z value gives correct probability for interval
```

## A Visual Clarification of Confidence Intervals

![A common source of confusion is why `qnorm(.975)` is used rather than `qnorm(.95)` to find the 95% confidence interval. This is because the normal distribution is symmetric and our confidence interval should cover the middle 95% of the distribution:](images/clipboard-3989755348.png)

![![](images/clipboard-390637980.png)](images/ci95.png)

## A Monte Carlo Simulation for Confidence Intervals

-   We can run a Monte Carlo simulation to confirm that a 95% confidence interval contains the true value of p 95% of the time.

-   A plot of confidence intervals from this simulation demonstrates that most intervals include p, but roughly 5% of intervals miss the true value of p.

### Code: Monte Carlo simulation

Note that to compute the exact 95% confidence interval, we would use `qnorm(.975)*SE_hat` instead of`2*SE_hat`.

```{r}
B <- 10000
inside <- replicate(B, {
    X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    X_hat <- mean(X)
    SE_hat <- sqrt(X_hat*(1-X_hat)/N)
    between(p, X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # TRUE if p in confidence interval
})
mean(inside)
```

![](images/clipboard-2194538514.png)

## The Correct Language

-   The 95% confidence intervals are random, but p is not random.

-   95% refers to the probability that the random interval falls on top of p.

-   It is technically incorrect to state that p has a 95% chance of being in between two values because that implies p is random.

## Power

-   If we are trying to predict the result of an election, then a confidence interval that includes a spread of 0 (a tie) is not helpful.

-   A confidence interval that includes a spread of 0 does not imply a close election, it means the sample size is too small.

-   Power is the probability of detecting an effect when there is a true effect to find. Power increases as sample size increases, because larger sample size means smaller standard error.

### Code: Confidence interval for the spread with sample size of 25

Note that to compute the exact 95% confidence interval, we would use `c(-qnorm(.975), qnorm(.975))` instead of 1.96.

```{r}
N <- 25
X_hat <- 0.48
(2*X_hat - 1) + c(-2, 2)*2*sqrt(X_hat*(1-X_hat)/N)
```

## p-Values

-   The null hypothesis is the hypothesis that there is no effect. In this case, the null hypothesis is that the spread is 0, or p=0.5.

```{=html}
<!-- -->
```
-   The p-value is the probability of detecting an effect of a certain size or larger when the null hypothesis is true.

```{=html}
<!-- -->
```
-   We can convert the probability of seeing an observed value under the null hypothesis into a standard normal random variable. We compute the value of that corresponds to the observed result, and then use that to compute the p-value.

```{=html}
<!-- -->
```
-   If a 95% confidence interval does not include our observed value, then the p-value must be smaller than 0.05.

```{=html}
<!-- -->
```
-   It is preferable to report confidence intervals instead of p-values, as confidence intervals give information about the size of the estimate and p-values do not.

### Code: Computing a p-value for observed spread of 0.02

```{r}
N <- 100    # sample size
z <- sqrt(N) * 0.02/0.5    # spread of 0.02
1 - (pnorm(z) - pnorm(-z))
```

## Another Explanation of p-Values

The p-value is the probability of observing a value as extreme or more extreme than the result given that the null hypothesis is true.

In the context of the normal distribution, this refers to the probability of observing a Z-score whose absolute value is as high or higher than the Z-score of interest.

Suppose we want to find the p-value of an observation 2 standard deviations larger than the mean. This means we are looking for anything with ![](images/clipboard-3128431914.png){width="42"}.

Graphically, the p-value gives the probability of an observation that's at least as far away from the mean or further. This plot shows a standard normal distribution (centered at z=0 with a standard deviation of 1). The shaded tails are the region of the graph that are 2 standard deviations or more away from the mean.

![The right tail can be found with `1-pnorm(2)`. We want to have both tails, though, because we want to find the probability of any observation as far away from the mean or farther, in either direction. (This is what's meant by a two-tailed p-value.) Because the distribution is symmetrical, the right and left tails are the same size and we know that our desired value is just `2*(1-pnorm(2))`.](images/pvalue-normal-dist.jpg)

Recall that, by default, `pnorm()` gives the CDF for a normal distribution with a mean of ![](images/clipboard-1238477884.png){width="36" height="17"} and standard deviation of ![](images/clipboard-3422139651.png){width="36" height="12"}. To find p-values for a given z-score `z` in a normal distribution with mean mu and standard deviation `sigma,` use `2*(1-pnorm(z, mu, sigma))` instead.

# Assessment 3.1: Confidence Intervals and p-Values

## Exercise 1. Confidence interval for p

For the following exercises, we will use actual poll data from the 2016 election. The exercises will contain pre-loaded data from the `dslabs` package.

```{r}
library(dslabs)
data("polls_us_election_2016")
```

We will use all the national polls that ended within a few weeks before the election.

Assume there are only two candidates and construct a 95% confidence interval for the election night proportion p.

-   Use `filter` to subset the data set for the poll data you want. Include polls that ended on or after October 31, 2016 (`enddate`). Only include polls that took place in the United States. Call this filtered object `polls`.

-   Use `nrow` to make sure you created a filtered object `polls` that contains the correct number of rows.

-   Extract the sample size `N` from the first poll in your subset object `polls`.

-   Convert the percentage of Clinton voters (`rawpoll_clinton`) from the first poll in `polls` to a proportion, `X_hat`. Print this value to the console.

-   Find the standard error of `X_hat` given `N`. Print this result to the console.

-   Calculate the 95% confidence interval of this estimate using the `qnorm` function.

-   Save the lower and upper confidence intervals as an object called `ci`. Save the lower confidence interval first.

```{r}
# Load the data
library(dslabs)
library(dplyr)
data(polls_us_election_2016)
head(polls_us_election_2016)

# Generate an object `polls` that contains data filtered for polls that ended on or after October 31, 2016 in the United States
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.")

# How many rows does `polls` contain? Print this value to the console.
nrow(polls)

# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- polls$samplesize[1]
N

# For the first poll in `polls`, assign the estimated percentage of Clinton voters to a variable called `X_hat`. Print this value to the console.
X_hat <- polls$rawpoll_clinton[1]/100
X_hat

# Calculate the standard error of `X_hat` and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(X_hat * (1-X_hat)/N)

# Use `qnorm` to calculate the 95% confidence interval for the proportion of Clinton voters. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(X_hat - qnorm(0.975)*se_hat, X_hat + qnorm(0.975)*se_hat)
ci
```

## Exercise 2. Pollster results for p

Create a new object called `pollster_results` that contains the pollster's name, the end date of the poll, the proportion of voters who declared a vote for Clinton, the standard error of this estimate, and the lower and upper bounds of the confidence interval for the estimate.

-   Use the `mutate` function to define four new columns: `X_hat`, `se_hat`, `lower`, and `upper`. Temporarily add these columns to the `polls` object that has already been loaded for you.

-   In the `X_hat` column, convert the raw poll results for Clinton to a proportion.

-   In the `se_hat` column, calculate the standard error of `X_hat` for each poll using the `sqrt` function.

-   In the `lower` column, calculate the lower bound of the 95% confidence interval using the `qnorm` function.

-   In the `upper` column, calculate the upper bound of the 95% confidence interval using the `qnorm` function.

-   Use the `select` function to select the columns from `polls` to save to the new object `pollster_results`.

```{r}
# The `polls` object that filtered all the data by date and nation has already been loaded. Examine it using the `head` function.
head(polls)

# Create a new object called `pollster_results` that contains columns for pollster name, end date, X_hat, se_hat, lower confidence interval, and upper confidence interval for each poll.
pollster_results <- polls %>%
  mutate(X_hat = polls$rawpoll_clinton/100,
         se_hat = sqrt(X_hat*(1-X_hat)/samplesize),
         lower = X_hat - qnorm(0.975)*se_hat,
         upper = X_hat + qnorm(0.975)*se_hat) %>%
  select(pollster, enddate, X_hat, se_hat, lower, upper)


pollster_results
```

## Exercise 3. Comparing to actual results - p

The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column called `hit` to `pollster_results`
that states if the confidence interval included the true proportion p=0.482 or not. What proportion of confidence intervals included p?

-   Use the `mutate` function to define a new variable called ‘hit’.

-   Use logical expressions to determine if each values in `lower` and `upper` span the actual proportion.

-   Use the `mean` function to determine the average value in `hit` and `summarize` the results using summarize.

-   Save the result as an object called `avg_hit`.

```{r}
# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)

# Add a logical variable called `hit` that indicates whether the actual value exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
p <- rep(0.482, 70)
avg_hit <- pollster_results %>%
  mutate(hit = between(p, lower, upper)) %>%
  summarize(mean(hit))
avg_hit

#or
avg_hit <- pollster_results %>% mutate(hit=(lower<0.482 & upper>0.482)) %>% summarize(mean(hit))
avg_hit
```

## Exercise 4. Theory of confidence intervals

If these confidence intervals are constructed correctly, and the theory holds up, what proportion of confidence intervals should include p?

Ans. 0.95

They were made using the margin of error...

## Exercise 5. Confidence interval for d

A much smaller proportion of the polls than expected produce confidence intervals containing p. Notice that most polls that fail to include p are underestimating. The rationale for this is that undecided voters historically divide evenly between the two main candidates on election day.

In this case, it is more informative to estimate the spread or the difference between the proportion of two candidates d, or 0.482−0.461=0.021 for this election.

Assume that there are only two parties and that  d=2p−1. Construct a 95% confidence interval for difference in proportions on election night.

-   Use the `mutate` function to define a new variable called ‘d_hat’ in `polls`. The new variable subtract the proportion of Trump voters from the proportion of Clinton voters.

-   Extract the sample size `N` from the first poll in your subset object `polls`.

-   Extract the difference in proportions of voters `d_hat` from the first poll in your subset object polls.

-   Use the formula above to calculate p from `d_hat`. Assign p to the variable `X_hat`.

-   Find the standard error of the spread given `N`.

-   Calculate the 95% confidence interval of this estimate of the difference in proportions, `d_hat`, using the `qnorm` function.

-   Save the lower and upper confidence intervals as an object called `ci`. Save the lower confidence interval first.

```{r}
# Add a statement to this line of code that will add a new column named `d_hat` to `polls`. The new column should contain the difference in the proportion of voters.
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.") %>% mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100)


# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- polls$samplesize[1]
N

# Assign the difference `d_hat` of the first poll in `polls` to a variable called `d_hat`. Print this value to the console.
d_hat <- polls$d_hat[1]
d_hat

# Assign proportion of votes for Clinton to the variable `X_hat`.
X_hat <- (d_hat+1)/2
X_hat

# Calculate the standard error of the spread and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- 2*sqrt(X_hat*(1-X_hat)/N)
se_hat 

# Use `qnorm` to calculate the 95% confidence interval for the difference in the proportions of voters. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(d_hat - qnorm(0.975)*se_hat, d_hat + qnorm(0.975)*se_hat)
ci
```

## Exercise 6. Pollster results for d

Create a new object called `pollster_results` that contains the pollster's name, the end date of the poll, the difference in the proportion of voters who declared a vote either, and the lower and upper bounds of the confidence interval for the estimate.

-   Use the `mutate` function to define four new columns: 'X_hat', 'se_hat', 'lower', and 'upper'. Temporarily add these columns to the `polls` object that has already been loaded for you.

-   In the `X_hat` column, calculate the proportion of voters for Clinton using `d_hat`.

-   In the `se_hat` column, calculate the standard error of the spread for each poll using the `sqrt` function.

-   In the `lower` column, calculate the lower bound of the 95% confidence interval using the `qnorm` function.

-   In the `upper` column, calculate the upper bound of the 95% confidence interval using the `qnorm` function.

-   Use the `select` function to select the `pollster, enddate, d_hat, lower, upper` columns from `polls` to save to the new object `pollster_results`.

```{r}
pollster_results <- polls %>%
  mutate(
    X_hat = (d_hat+1)/2,
    se_hat = sqrt(X_hat*(1-X_hat)/.$samplesize),
    lower = d_hat - qnorm(0.975)*se_hat,
    upper = d_hat + qnorm(0.975)*se_hat
  ) %>%
  select(pollster, enddate, d_hat, lower, upper)
pollster_results
```

## Exercise 7. Comparing to actual results - d

What proportion of confidence intervals for the difference between the proportion of voters included d, the actual difference in election day?

-   Use the `mutate` function to define a new variable within`pollster_results` called hit.

-   Use logical expressions to determine if each values in `lower` and `upper` span the actual difference in proportions of voters.

-   Use the `mean` function to determine the average value in `hit` and summarize the results using `summarize`.

-   Save the result as an object called `avg_hit`.

```{r}
# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)

# Add a logical variable called `hit` that indicates whether the actual value (0.021) exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
avg_hit <- pollster_results %>% 
  mutate(hit=(lower<0.021 & upper>0.021)) %>%
  summarize(mean(hit))

avg_hit
```

## Exercise 8. Comparing to actual results by pollster

Although the proportion of confidence intervals that include the actual difference between the proportion of voters increases substantially, it is still lower that 0.95. In the next chapter, we learn the reason for this.

-   Define a new variable `errors` that contains the difference between the estimated difference between the proportion of voters and the actual difference on election day, 0.021.

-   To create the plot of errors by pollster, add a layer with the function `geom_point`. The aesthetic mappings require a definition of the x-axis and y-axis variables. So the code looks like the example below, but you fill in the variables for x and y.

-   The last line of the example code adjusts the x-axis labels so that they are easier to read.

```{r}
# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)
library(ggplot2)

# Add variable called `error` to the object `polls` that contains the difference between d_hat and the actual difference on election day. Then make a plot of the error stratified by pollster.
polls %>% 
  mutate(error = d_hat - 0.021) %>%
  ggplot(aes(x = pollster, y = error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

## Exercise 9. Comparing to actual results by pollster - multiple polls

Remake the plot you made for the previous exercise, but only for pollsters that took five or more polls.

You can use dplyr tools `group_by` and `n` to group data by a variable of interest and then count the number of observations in the groups. The function `filter` filters data piped into it by your specified condition.

For example:

```{r}
data %>% group_by(variable_for_grouping) 
    %>% filter(n() >= 5)
```

-   Define a new variable `errors` that contains the difference between the estimated difference between the proportion of voters and the actual difference on election day, 0.021.

-   Group the data by pollster using the `group_by` function.

-   Filter the data by pollsters with 5 or more polls.

-   Use `ggplo`t to create the plot of errors by pollster.

-   Add a layer with the function `geom_point`.

```{r}
# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)

# Add variable called `error` to the object `polls` that contains the difference between d_hat and the actual difference on election day. Then make a plot of the error stratified by pollster, but only for pollsters who took 5 or more polls.
polls %>% 
  mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(x = pollster, y = error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
